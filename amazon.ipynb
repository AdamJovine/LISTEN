{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6888398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_jsonl_file(filepath, max_lines=None):\n",
    "    \"\"\"\n",
    "    Load a JSONL file line by line.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the JSONL file\n",
    "        max_lines: Maximum number of lines to read (for testing)\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_lines and i >= max_lines:\n",
    "                break\n",
    "            try:\n",
    "                data.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line {i+1}: {e}\")\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "def filter_prolific_reviewers(reviews_data, min_reviews=20):\n",
    "    \"\"\"\n",
    "    Filter reviews to only include those from users with >= min_reviews.\n",
    "    \n",
    "    Args:\n",
    "        reviews_data: List of review dictionaries\n",
    "        min_reviews: Minimum number of reviews a user must have\n",
    "    \n",
    "    Returns:\n",
    "        Filtered list of reviews\n",
    "    \"\"\"\n",
    "    # Count reviews per user\n",
    "    print(\"Counting reviews per user...\")\n",
    "    user_review_counts = Counter(review['user_id'] for review in reviews_data)\n",
    "    \n",
    "    # Get users with >= min_reviews\n",
    "    prolific_users = {user_id for user_id, count in user_review_counts.items() \n",
    "                      if count >= min_reviews}\n",
    "    \n",
    "    print(f\"Found {len(prolific_users)} users with >= {min_reviews} reviews\")\n",
    "    print(f\"Total users: {len(user_review_counts)}\")\n",
    "    \n",
    "    # Filter reviews\n",
    "    filtered_reviews = [review for review in reviews_data \n",
    "                        if review['user_id'] in prolific_users]\n",
    "    \n",
    "    print(f\"Filtered reviews: {len(filtered_reviews)} out of {len(reviews_data)}\")\n",
    "    \n",
    "    return filtered_reviews\n",
    "\n",
    "def create_product_lookup(products_data):\n",
    "    \"\"\"\n",
    "    Create a dictionary for fast product lookup by parent_asin.\n",
    "    \n",
    "    Args:\n",
    "        products_data: List of product dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping parent_asin to product data\n",
    "    \"\"\"\n",
    "    product_lookup = {}\n",
    "    for product in products_data:\n",
    "        parent_asin = product.get('parent_asin')\n",
    "        if parent_asin:\n",
    "            product_lookup[parent_asin] = product\n",
    "    return product_lookup\n",
    "\n",
    "def merge_reviews_with_products(reviews_data, products_data):\n",
    "    \"\"\"\n",
    "    Merge review data with product metadata.\n",
    "    Only includes rows with all required fields (no missing data).\n",
    "    \n",
    "    Args:\n",
    "        reviews_data: List of review dictionaries\n",
    "        products_data: List of product dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        Pandas DataFrame with merged data (no missing required fields)\n",
    "    \"\"\"\n",
    "    print(\"Creating product lookup...\")\n",
    "    product_lookup = create_product_lookup(products_data)\n",
    "    \n",
    "    print(\"Merging reviews with products...\")\n",
    "    merged_data = []\n",
    "    skipped_missing_data = 0\n",
    "    skipped_no_product = 0\n",
    "    \n",
    "    for review in tqdm(reviews_data, desc=\"Merging data\"):\n",
    "        # Get the parent_asin from the review\n",
    "        parent_asin = review.get('parent_asin')\n",
    "        \n",
    "        if parent_asin and parent_asin in product_lookup:\n",
    "            product = product_lookup[parent_asin]\n",
    "            \n",
    "            # Check required fields before creating record\n",
    "            # Required fields: product_title, review_rating, product_features, product_price\n",
    "            product_title = product.get('title')\n",
    "            review_rating = review.get('rating')\n",
    "            product_features = product.get('features')\n",
    "            product_price = product.get('price')\n",
    "            \n",
    "            # Skip if any required field is missing or None\n",
    "            if not all([\n",
    "                product_title is not None and product_title != '',\n",
    "                review_rating is not None,\n",
    "                product_features is not None and len(product_features) > 0,\n",
    "                product_price is not None and product_price != ''\n",
    "            ]):\n",
    "                skipped_missing_data += 1\n",
    "                continue\n",
    "            \n",
    "            # Create merged record\n",
    "            merged_record = {\n",
    "                # Review fields\n",
    "                'review_rating': review_rating,\n",
    "                'review_title': review.get('title'),\n",
    "                'review_text': review.get('text'),\n",
    "                'user_id': review.get('user_id'),\n",
    "                'timestamp': review.get('timestamp'),\n",
    "                'helpful_vote': review.get('helpful_vote'),\n",
    "                'verified_purchase': review.get('verified_purchase'),\n",
    "                'asin': review.get('asin'),\n",
    "                \n",
    "                # Product fields (required)\n",
    "                'product_title': product_title,\n",
    "                'product_features': ' | '.join(product_features),\n",
    "                'product_price': product_price,\n",
    "                \n",
    "                # Product fields (optional)\n",
    "                'product_category': product.get('main_category'),\n",
    "                'product_avg_rating': product.get('average_rating'),\n",
    "                'product_rating_count': product.get('rating_number'),\n",
    "                'product_brand': product.get('details', {}).get('Brand'),\n",
    "                'product_store': product.get('store'),\n",
    "                'parent_asin': parent_asin\n",
    "            }\n",
    "            \n",
    "            # Add product description if available\n",
    "            if product.get('description'):\n",
    "                merged_record['product_description'] = ' '.join(product['description'])\n",
    "            else:\n",
    "                merged_record['product_description'] = None\n",
    "            \n",
    "            merged_data.append(merged_record)\n",
    "        else:\n",
    "            skipped_no_product += 1\n",
    "    \n",
    "    print(f\"Successfully merged {len(merged_data)} records\")\n",
    "    print(f\"Skipped {skipped_missing_data} records due to missing required fields\")\n",
    "    print(f\"Skipped {skipped_no_product} records due to no matching product\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(merged_data)\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', errors='coerce')\n",
    "    \n",
    "    # Final verification - ensure no nulls in required columns\n",
    "    required_cols = ['product_title', 'review_rating', 'product_features', 'product_price']\n",
    "    for col in required_cols:\n",
    "        if col in df.columns:\n",
    "            null_count = df[col].isnull().sum()\n",
    "            if null_count > 0:\n",
    "                print(f\"WARNING: Found {null_count} null values in {col}\")\n",
    "                df = df.dropna(subset=[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main(reviews_filepath='input/All_Beauty.jsonl', \n",
    "         products_filepath='input/meta_All_Beauty.jsonl',\n",
    "         min_reviews=20,\n",
    "         sample_size=None):\n",
    "    \"\"\"\n",
    "    Main function to process and merge beauty product data.\n",
    "    Only saves rows with all required fields (no missing data).\n",
    "    \n",
    "    Args:\n",
    "        reviews_filepath: Path to reviews JSONL file\n",
    "        products_filepath: Path to products metadata JSONL file\n",
    "        min_reviews: Minimum number of reviews for a user to be included\n",
    "        sample_size: If provided, only load this many lines (for testing)\n",
    "    \n",
    "    Returns:\n",
    "        Merged DataFrame with complete data only\n",
    "    \"\"\"\n",
    "    print(f\"Loading reviews from {reviews_filepath}...\")\n",
    "    reviews_data = load_jsonl_file(reviews_filepath, max_lines=sample_size)\n",
    "    print(f\"Loaded {len(reviews_data)} reviews\")\n",
    "    \n",
    "    print(f\"\\nLoading products from {products_filepath}...\")\n",
    "    products_data = load_jsonl_file(products_filepath, max_lines=sample_size)\n",
    "    print(f\"Loaded {len(products_data)} products\")\n",
    "    \n",
    "    print(f\"\\nFiltering for users with >= {min_reviews} reviews...\")\n",
    "    filtered_reviews = filter_prolific_reviewers(reviews_data, min_reviews)\n",
    "    \n",
    "    print(\"\\nMerging data (only complete records)...\")\n",
    "    df = merge_reviews_with_products(filtered_reviews, products_data)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Final DataFrame shape: {df.shape}\")\n",
    "    print(f\"Unique users: {df['user_id'].nunique()}\")\n",
    "    print(f\"Unique products: {df['parent_asin'].nunique()}\")\n",
    "    print(f\"\\nDataFrame columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Verify required fields have no missing data\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"REQUIRED FIELDS VERIFICATION\")\n",
    "    print(\"=\"*50)\n",
    "    required_fields = ['product_title', 'review_rating', 'product_features', 'product_price']\n",
    "    for field in required_fields:\n",
    "        if field in df.columns:\n",
    "            non_null = df[field].notna().sum()\n",
    "            total = len(df)\n",
    "            print(f\"{field}: {non_null}/{total} non-null ({non_null/total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df[['product_title', 'review_rating', 'product_features', 'product_price']].head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def parse(data_type):\n",
    "    # For testing with a small sample\n",
    "    # df = main(sample_size=10000, min_reviews=20)\n",
    "    \n",
    "    # For full processing\n",
    "    #reviews_filepath='input/All_Beauty.jsonl', \n",
    "    #     products_filepath \n",
    "    \n",
    "    if data_type == 'beauty':\n",
    "        df = main(min_reviews=20)\n",
    "    \n",
    "        # Save to CSV\n",
    "        output_file = 'input/beauty_merged_data.csv'\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nData saved to {output_file}\")\n",
    "        \n",
    "        # Display some statistics\n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        print(f\"Average review rating: {df['review_rating'].mean():.2f}\")\n",
    "        print(f\"Reviews per user: {df.groupby('user_id').size().mean():.1f}\")\n",
    "        print(f\"Reviews per product: {df.groupby('parent_asin').size().mean():.1f}\")\n",
    "        print(f\"Verified purchases: {df['verified_purchase'].sum()} ({df['verified_purchase'].mean()*100:.1f}%)\")\n",
    "    if data_type == 'industrial':\n",
    "        df = main(reviews_filepath='input/Industrial_and_Scientific.jsonl',products_filepath='input/meta_Industrial_and_Scientific.jsonl',min_reviews=20)\n",
    "    \n",
    "        # Save to CSV\n",
    "        output_file = 'input/ind_merged_data.csv'\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "    if data_type == 'electronic':\n",
    "        df = main(reviews_filepath='input/Electronics.jsonl',products_filepath='input/meta_Electronics.jsonl',min_reviews=20)\n",
    "    \n",
    "        # Save to CSV\n",
    "        output_file = 'input/electronic_merged_data.csv'\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "parse('electronic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c7349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing users:  45%|████▍     | 58558/130231 [15:43<15:26, 77.35it/s] "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def create_train_test_split(df, first_n_reviews=10):\n",
    "    \"\"\"\n",
    "    Split data into train and test sets where:\n",
    "    - Train: First N reviews (by timestamp) for each user\n",
    "    - Test: Positive/negative review pairs from remaining reviews\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with merged review and product data\n",
    "        first_n_reviews: Number of first reviews to use for training\n",
    "    \n",
    "    Returns:\n",
    "        train_df: Training DataFrame\n",
    "        test_df: Test DataFrame with positive/negative pairs\n",
    "        test_pairs: List of dictionaries containing pair information\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure timestamp is datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Sort by user and timestamp\n",
    "    df_sorted = df.sort_values(['user_id', 'timestamp'])\n",
    "    \n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    test_pairs = []\n",
    "    \n",
    "    print(\"Processing users to create train/test split...\")\n",
    "    \n",
    "    # Group by user\n",
    "    user_groups = df_sorted.groupby('user_id')\n",
    "    \n",
    "    for user_id, user_reviews in tqdm(user_groups, desc=\"Processing users\"):\n",
    "\n",
    "        user_reviews = user_reviews.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        # First N reviews go to training\n",
    "        train_reviews = user_reviews.iloc[:first_n_reviews]\n",
    "        train_data.append(train_reviews)\n",
    "        \n",
    "        # Remaining reviews for potential test pairs\n",
    "        remaining_reviews = user_reviews.iloc[first_n_reviews:]\n",
    "        \n",
    "        if len(remaining_reviews) > 0:\n",
    "            # Find positive reviews (rating >= 4)\n",
    "            positive_reviews = remaining_reviews[remaining_reviews['review_rating'] >= 4]\n",
    "            \n",
    "            # Find negative reviews (rating <= 2)\n",
    "            negative_reviews = remaining_reviews[remaining_reviews['review_rating'] <= 2]\n",
    "            \n",
    "            # Create pairs of positive and negative reviews\n",
    "            if len(positive_reviews) > 0 and len(negative_reviews) > 0:\n",
    "                # For each positive review, pair with each negative review\n",
    "                for _, pos_review in positive_reviews.iterrows():\n",
    "                    for _, neg_review in negative_reviews.iterrows():\n",
    "                        # Add both reviews to test set\n",
    "                        test_data.append(pos_review.to_frame().T)\n",
    "                        test_data.append(neg_review.to_frame().T)\n",
    "                        #print('pr' , pos_review)\n",
    "                        # Store pair information\n",
    "                        pair_info = {\n",
    "                            'user_id': user_id,\n",
    "                            'positive_review': {\n",
    "                                'asin': pos_review['asin'],\n",
    "                                'parent_asin': pos_review['parent_asin'],\n",
    "                                'product_title': pos_review['product_title'],\n",
    "                                'product_features' : pos_review['product_features'], \n",
    "                                'product_price' : pos_review['product_price'],\n",
    "                                'rating': pos_review['review_rating'],\n",
    "                                'timestamp': pos_review['timestamp'].isoformat() if pd.notna(pos_review['timestamp']) else None,\n",
    "                                'review_text': pos_review['review_text']\n",
    "                            },\n",
    "                            'negative_review': {\n",
    "                                'asin': neg_review['asin'],\n",
    "                                'parent_asin': neg_review['parent_asin'],\n",
    "                                'product_title': neg_review['product_title'],\n",
    "                                'product_features' : neg_review['product_features'], \n",
    "                                'product_price' : neg_review['product_price'], \n",
    "                                'rating': neg_review['review_rating'],\n",
    "                                'timestamp': neg_review['timestamp'].isoformat() if pd.notna(neg_review['timestamp']) else None,\n",
    "                                'review_text': neg_review['review_text']\n",
    "                            }\n",
    "                        }\n",
    "                        test_pairs.append(pair_info)\n",
    "    \n",
    "    # Combine all training data\n",
    "    if train_data:\n",
    "        train_df = pd.concat(train_data, ignore_index=True)\n",
    "    else:\n",
    "        train_df = pd.DataFrame()\n",
    "    \n",
    "    # Combine all test data and remove duplicates\n",
    "    if test_data:\n",
    "        test_df = pd.concat(test_data, ignore_index=True)\n",
    "        # Remove duplicate reviews (same review might be in multiple pairs)\n",
    "        test_df = test_df.drop_duplicates(subset=['user_id', 'asin', 'timestamp'])\n",
    "    else:\n",
    "        test_df = pd.DataFrame()\n",
    "    \n",
    "    print(f\"\\nSplit complete!\")\n",
    "    print(f\"Training set: {len(train_df)} reviews\")\n",
    "    print(f\"Test set: {len(test_df)} reviews\")\n",
    "    print(f\"Test pairs: {len(test_pairs)} positive/negative pairs\")\n",
    "    \n",
    "    return train_df, test_df, test_pairs\n",
    "\n",
    "def analyze_split(train_df, test_df, test_pairs):\n",
    "    \"\"\"\n",
    "    Analyze the train/test split to provide statistics.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training DataFrame\n",
    "        test_df: Test DataFrame\n",
    "        test_pairs: List of test pairs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with statistics\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    # Training statistics\n",
    "    stats['train'] = {\n",
    "        'total_reviews': len(train_df),\n",
    "        'unique_users': train_df['user_id'].nunique() if len(train_df) > 0 else 0,\n",
    "        'unique_products': train_df['parent_asin'].nunique() if len(train_df) > 0 else 0,\n",
    "        'avg_rating': train_df['review_rating'].mean() if len(train_df) > 0 else 0,\n",
    "        'rating_distribution': train_df['review_rating'].value_counts().to_dict() if len(train_df) > 0 else {}\n",
    "    }\n",
    "    \n",
    "    # Test statistics\n",
    "    stats['test'] = {\n",
    "        'total_reviews': len(test_df),\n",
    "        'unique_users': test_df['user_id'].nunique() if len(test_df) > 0 else 0,\n",
    "        'unique_products': test_df['parent_asin'].nunique() if len(test_df) > 0 else 0,\n",
    "        'avg_rating': test_df['review_rating'].mean() if len(test_df) > 0 else 0,\n",
    "        'rating_distribution': test_df['review_rating'].value_counts().to_dict() if len(test_df) > 0 else {},\n",
    "        'total_pairs': len(test_pairs)\n",
    "    }\n",
    "    \n",
    "    # Users in both sets\n",
    "    if len(train_df) > 0 and len(test_df) > 0:\n",
    "        train_users = set(train_df['user_id'].unique())\n",
    "        test_users = set(test_df['user_id'].unique())\n",
    "        stats['overlap'] = {\n",
    "            'users_in_both': len(train_users.intersection(test_users)),\n",
    "            'users_only_train': len(train_users - test_users),\n",
    "            'users_only_test': len(test_users - train_users)\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def save_splits(train_df, test_df, test_pairs, output_prefix='beauty_split'):\n",
    "    \"\"\"\n",
    "    Save the train/test splits and pairs information to files.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Training DataFrame\n",
    "        test_df: Test DataFrame\n",
    "        test_pairs: List of test pairs\n",
    "        output_prefix: Prefix for output filenames\n",
    "    \"\"\"\n",
    "    # Save training data\n",
    "    train_file = f'input/{output_prefix}_train.csv'\n",
    "    train_df.to_csv(train_file, index=False)\n",
    "    print(f\"Training data saved to {train_file}\")\n",
    "    \n",
    "    # Save test data\n",
    "    test_file = f'input/{output_prefix}_test.csv'\n",
    "    test_df.to_csv(test_file, index=False)\n",
    "    print(f\"Test data saved to {test_file}\")\n",
    "    \n",
    "    # Save test pairs information\n",
    "    pairs_file = f'input/{output_prefix}_test_pairs.json'\n",
    "    with open(pairs_file, 'w') as f:\n",
    "        json.dump(test_pairs, f, indent=2, default=str)\n",
    "    print(f\"Test pairs information saved to {pairs_file}\")\n",
    "    \n",
    "    # Save statistics\n",
    "    stats = analyze_split(train_df, test_df, test_pairs)\n",
    "    stats_file = f'input/{output_prefix}_statistics.json'\n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(stats, f, indent=2, default=str)\n",
    "    print(f\"Statistics saved to {stats_file}\")\n",
    "\n",
    "def create_preference_dataset(test_pairs, output_file='preference_pairs.jsonl'):\n",
    "    \"\"\"\n",
    "    Create a preference dataset from test pairs in a format suitable for preference learning.\n",
    "    \n",
    "    Args:\n",
    "        test_pairs: List of test pairs\n",
    "        output_file: Output filename for preference data\n",
    "    \"\"\"\n",
    "    preference_data = []\n",
    "    \n",
    "    for pair in test_pairs:\n",
    "        # Create a preference record\n",
    "        pref_record = {\n",
    "            'user_id': pair['user_id'],\n",
    "            'preferred': {\n",
    "                'product_id': pair['positive_review']['parent_asin'],\n",
    "                'product_title': pair['positive_review']['product_title'],\n",
    "                'product_price': pair['positive_review']['product_price'],\n",
    "                'product_features': pair['positive_review']['product_features'],\n",
    "                'review_text': pair['positive_review']['review_text'],\n",
    "                'rating': pair['positive_review']['rating'],\n",
    "                'timestamp': pair['positive_review']['timestamp']\n",
    "            },\n",
    "            'rejected': {\n",
    "                'product_id': pair['negative_review']['parent_asin'],\n",
    "                'product_title': pair['negative_review']['product_title'],\n",
    "                'product_price': pair['positive_review']['product_price'],\n",
    "                'product_features': pair['positive_review']['product_features'],\n",
    "                'review_text': pair['negative_review']['review_text'],\n",
    "                'rating': pair['negative_review']['rating'],\n",
    "                'timestamp': pair['negative_review']['timestamp']\n",
    "            }\n",
    "        }\n",
    "        preference_data.append(pref_record)\n",
    "    \n",
    "    # Save as JSONL for easy streaming\n",
    "    with open(output_file, 'w') as f:\n",
    "        for record in preference_data:\n",
    "            f.write(json.dumps(record) + '\\n')\n",
    "    \n",
    "    print(f\"Preference dataset saved to {output_file}\")\n",
    "    return preference_data\n",
    "\n",
    "def main(input_file='beauty_merged_data.csv', first_n_reviews=5 ):\n",
    "    \"\"\"\n",
    "    Main function to create train/test split with preference pairs.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to the merged data CSV file\n",
    "        first_n_reviews: Number of first reviews per user for training\n",
    "    \"\"\"\n",
    "    output_prefix = input_file[:4]\n",
    "    print(f\"Loading data from {input_file}...\")\n",
    "    df = pd.read_csv('input/' + input_file)\n",
    "    print(f\"Loaded {len(df)} reviews from {df['user_id'].nunique()} users\")\n",
    "    \n",
    "    # Create train/test split\n",
    "    train_df, test_df, test_pairs = create_train_test_split(df, first_n_reviews)\n",
    "    \n",
    "    # Analyze the split\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SPLIT ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    stats = analyze_split(train_df, test_df, test_pairs)\n",
    "    \n",
    "    print(\"\\nTraining Set:\")\n",
    "    print(f\"  - Total reviews: {stats['train']['total_reviews']}\")\n",
    "    print(f\"  - Unique users: {stats['train']['unique_users']}\")\n",
    "    print(f\"  - Unique products: {stats['train']['unique_products']}\")\n",
    "    print(f\"  - Average rating: {stats['train']['avg_rating']:.2f}\")\n",
    "    \n",
    "    print(\"\\nTest Set:\")\n",
    "    print(f\"  - Total reviews: {stats['test']['total_reviews']}\")\n",
    "    print(f\"  - Unique users: {stats['test']['unique_users']}\")\n",
    "    print(f\"  - Unique products: {stats['test']['unique_products']}\")\n",
    "    print(f\"  - Average rating: {stats['test']['avg_rating']:.2f}\")\n",
    "    print(f\"  - Total preference pairs: {stats['test']['total_pairs']}\")\n",
    "    \n",
    "    if 'overlap' in stats:\n",
    "        print(\"\\nUser Overlap:\")\n",
    "        print(f\"  - Users in both sets: {stats['overlap']['users_in_both']}\")\n",
    "        print(f\"  - Users only in train: {stats['overlap']['users_only_train']}\")\n",
    "        print(f\"  - Users only in test: {stats['overlap']['users_only_test']}\")\n",
    "    \n",
    "    # Save the splits\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SAVING FILES\")\n",
    "    print(\"=\"*50)\n",
    "    print('TTT' , test_pairs)\n",
    "    save_splits(train_df, test_df, test_pairs, output_prefix=output_prefix)\n",
    "    \n",
    "    # Create preference dataset\n",
    "    if test_pairs:\n",
    "        create_preference_dataset(test_pairs)\n",
    "    \n",
    "    return train_df, test_df, test_pairs\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Run with default settings\n",
    "    train_df, test_df, test_pairs = main(input_file = 'electronic_merged_data.csv')\n",
    "    #train_df, test_df, test_pairs = main(input_file = 'beauty_merged_data.csv')\n",
    "    \n",
    "    # Or specify custom parameters\n",
    "    # train_df, test_df, test_pairs = main(\n",
    "    #     input_file='your_merged_data.csv',\n",
    "    #     first_n_reviews=10\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd17935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Import the prompt template (assumes it's in the same directory or installed)\n",
    "from prompt import BeautyPromptTemplateAdapter#BeautyPreferencePromptTemplate, BeautyPreferenceBatchProcessor\n",
    "\n",
    "# Import the LLM client\n",
    "from remoteOss import get_local_client\n",
    "\n",
    "\n",
    "class BeautyPreferenceLLMEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates LLM performance on beauty product preference prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_pairs_file: str = 'input/beauty_split_test_pairs.json',\n",
    "        model_id: str = None,\n",
    "        include_product_features: bool = True,\n",
    "        include_review_text: bool = True,\n",
    "        reasoning: bool = True,\n",
    "        max_review_text_length: int = 150,\n",
    "        temperature: float = 0.0,\n",
    "        max_new_tokens: int = 256,\n",
    "        seed: int = 42\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator.\n",
    "        \n",
    "        Args:\n",
    "            train_df: Training DataFrame with user reviews\n",
    "            test_pairs_file: Path to test pairs JSON file\n",
    "            model_id: Model ID for the LLM (None uses default)\n",
    "            include_product_features: Include features in prompts\n",
    "            include_review_text: Include review text in prompts\n",
    "            reasoning: Request reasoning from the LLM\n",
    "            max_review_text_length: Max length for review text\n",
    "            temperature: LLM temperature setting\n",
    "            max_new_tokens: Max tokens for LLM response\n",
    "            seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        # Initialize prompt template\n",
    "        self.prompt_template = BeautyPromptTemplateAdapter(\n",
    "            train_df=train_df,\n",
    "            test_pairs_file=test_pairs_file,\n",
    "            include_product_features=include_product_features,\n",
    "            include_review_text=include_review_text,\n",
    "            reasoning=reasoning,\n",
    "            max_review_text_length=max_review_text_length\n",
    "        )\n",
    "        \n",
    "        # Initialize LLM client\n",
    "        if model_id:\n",
    "            self.llm_client = get_local_client(model_id=model_id)\n",
    "        else:\n",
    "            self.llm_client = get_local_client()\n",
    "        \n",
    "        # LLM parameters\n",
    "        self.temperature = temperature\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = []\n",
    "        self.errors = []\n",
    "    \n",
    "    def evaluate_single_pair(self, pair_index: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate a single test pair.\n",
    "        \n",
    "        Args:\n",
    "            pair_index: Index of the test pair\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate prompt\n",
    "            prompt, pair_info = self.prompt_template.format_from_test_pair(pair_index)\n",
    "            print('PROMPT ' , prompt , pair_info)\n",
    "            # Get evaluation info (ground truth)\n",
    "            eval_info = self.prompt_template.evaluate_pair(pair_index)\n",
    "            \n",
    "            # Call LLM\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Format prompt for the LLM's call_oracle method\n",
    "            # Extract schedule-like data from the pair for compatibility\n",
    "            sched_a = {\n",
    "                'product': eval_info['product_a_title'],\n",
    "                'rating': eval_info['product_a_rating']\n",
    "            }\n",
    "            sched_b = {\n",
    "                'product': eval_info['product_b_title'],\n",
    "                'rating': eval_info['product_b_rating']\n",
    "            }\n",
    "            \n",
    "            choice, raw_response = self.llm_client.call_oracle(\n",
    "                prompt=prompt,\n",
    "                sched_a=sched_a,\n",
    "                sched_b=sched_b,\n",
    "                temperature=self.temperature,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                seed=self.seed,\n",
    "                stop=[\"===\", \"---\", \"\\n\\n\\n\"]  # Stop sequences to prevent rambling\n",
    "            )\n",
    "            \n",
    "            print('cho' , choice, 'raw ,' , raw_response)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Check if prediction is correct\n",
    "            is_correct = (choice == eval_info['ground_truth'])\n",
    "            \n",
    "            result = {\n",
    "                'pair_index': pair_index,\n",
    "                'user_id': eval_info['user_id'],\n",
    "                'predicted': choice,\n",
    "                'ground_truth': eval_info['ground_truth'],\n",
    "                'correct': is_correct,\n",
    "                'product_a': eval_info['product_a_title'],\n",
    "                'product_b': eval_info['product_b_title'],\n",
    "                'rating_a': eval_info['product_a_rating'],\n",
    "                'rating_b': eval_info['product_b_rating'],\n",
    "                'raw_response': raw_response,\n",
    "                'inference_time': inference_time,\n",
    "                'prompt_length': len(prompt),\n",
    "                'response_length': len(raw_response)\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error = {\n",
    "                'pair_index': pair_index,\n",
    "                'error': str(e),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "            self.errors.append(error)\n",
    "            print(f\"Error evaluating pair {pair_index}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def evaluate_batch(\n",
    "        self,\n",
    "        pair_indices: List[int] = None,\n",
    "        sample_size: int = None,\n",
    "        verbose: bool = True,\n",
    "        save_intermediate: bool = True,\n",
    "        output_dir: str = 'evaluation_results'\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate multiple test pairs.\n",
    "        \n",
    "        Args:\n",
    "            pair_indices: Specific indices to evaluate\n",
    "            sample_size: Random sample size (if pair_indices not provided)\n",
    "            verbose: Print progress\n",
    "            save_intermediate: Save results periodically\n",
    "            output_dir: Directory to save results\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        # Determine which pairs to evaluate\n",
    "        if pair_indices is None:\n",
    "            total_pairs = len(self.prompt_template.test_pairs)\n",
    "            if sample_size:\n",
    "                import random\n",
    "                pair_indices = random.sample(range(total_pairs), min(sample_size, total_pairs))\n",
    "            else:\n",
    "                pair_indices = range(total_pairs)\n",
    "        \n",
    "        # Create output directory\n",
    "        if save_intermediate:\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Evaluate each pair\n",
    "        for idx in tqdm(pair_indices, desc=\"Evaluating pairs\", disable=not verbose):\n",
    "            result = self.evaluate_single_pair(idx)\n",
    "            if result:\n",
    "                self.results.append(result)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            if save_intermediate and len(self.results) % 10 == 0:\n",
    "                self._save_intermediate_results(output_dir)\n",
    "        \n",
    "        # Final save\n",
    "        if save_intermediate:\n",
    "            self._save_intermediate_results(output_dir)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self.calculate_metrics()\n",
    "        \n",
    "        #if verbose:\n",
    "        #    self.print_metrics(metrics)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def calculate_metrics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate evaluation metrics from results.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with various metrics\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            return {'error': 'No results to evaluate'}\n",
    "        \n",
    "        df = pd.DataFrame(self.results)\n",
    "        \n",
    "        # Basic accuracy\n",
    "        accuracy = df['correct'].mean()\n",
    "        \n",
    "        # Per-user accuracy\n",
    "        user_accuracy = df.groupby('user_id')['correct'].agg(['mean', 'count'])\n",
    "        \n",
    "        # Rating difference analysis\n",
    "        df['rating_diff'] = df['rating_a'] - df['rating_b']\n",
    "        \n",
    "        # Accuracy by rating difference magnitude\n",
    "        df['rating_diff_abs'] = df['rating_diff'].abs()\n",
    "        accuracy_by_diff = df.groupby('rating_diff_abs')['correct'].agg(['mean', 'count'])\n",
    "        \n",
    "        # Inference time statistics\n",
    "        avg_inference_time = df['inference_time'].mean()\n",
    "        median_inference_time = df['inference_time'].median()\n",
    "        \n",
    "        # Response analysis\n",
    "        avg_response_length = df['response_length'].mean()\n",
    "        avg_prompt_length = df['prompt_length'].mean()\n",
    "        \n",
    "        metrics = {\n",
    "            'overall_accuracy': accuracy,\n",
    "            'total_evaluated': len(df),\n",
    "            'total_errors': len(self.errors),\n",
    "            'user_metrics': {\n",
    "                'unique_users': df['user_id'].nunique(),\n",
    "                'avg_accuracy_per_user': user_accuracy['mean'].mean(),\n",
    "                'std_accuracy_per_user': user_accuracy['mean'].std(),\n",
    "                'min_user_accuracy': user_accuracy['mean'].min(),\n",
    "                'max_user_accuracy': user_accuracy['mean'].max()\n",
    "            },\n",
    "            'rating_difference_metrics': {\n",
    "                'accuracy_by_diff': accuracy_by_diff.to_dict() if not accuracy_by_diff.empty else {},\n",
    "                'avg_rating_diff': df['rating_diff'].mean(),\n",
    "                'accuracy_when_diff_1': df[df['rating_diff_abs'] == 1]['correct'].mean() if len(df[df['rating_diff_abs'] == 1]) > 0 else None,\n",
    "                'accuracy_when_diff_2': df[df['rating_diff_abs'] == 2]['correct'].mean() if len(df[df['rating_diff_abs'] == 2]) > 0 else None,\n",
    "                'accuracy_when_diff_3': df[df['rating_diff_abs'] == 3]['correct'].mean() if len(df[df['rating_diff_abs'] == 3]) > 0 else None,\n",
    "                'accuracy_when_diff_4': df[df['rating_diff_abs'] == 4]['correct'].mean() if len(df[df['rating_diff_abs'] == 4]) > 0 else None,\n",
    "            },\n",
    "            'performance_metrics': {\n",
    "                'avg_inference_time': avg_inference_time,\n",
    "                'median_inference_time': median_inference_time,\n",
    "                'total_inference_time': df['inference_time'].sum(),\n",
    "                'avg_response_length': avg_response_length,\n",
    "                'avg_prompt_length': avg_prompt_length\n",
    "            },\n",
    "            'prediction_distribution': {\n",
    "                'predicted_A': (df['predicted'] == 'A').sum(),\n",
    "                'predicted_B': (df['predicted'] == 'B').sum(),\n",
    "                'ground_truth_A': (df['ground_truth'] == 'A').sum(),\n",
    "                'ground_truth_B': (df['ground_truth'] == 'B').sum()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add confusion matrix\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "        \n",
    "        try:\n",
    "            cm = confusion_matrix(df['ground_truth'], df['predicted'], labels=['A', 'B'])\n",
    "            metrics['confusion_matrix'] = {\n",
    "                'true_A_pred_A': int(cm[0, 0]),\n",
    "                'true_A_pred_B': int(cm[0, 1]),\n",
    "                'true_B_pred_A': int(cm[1, 0]),\n",
    "                'true_B_pred_B': int(cm[1, 1])\n",
    "            }\n",
    "            \n",
    "            # Add classification report\n",
    "            report = classification_report(\n",
    "                df['ground_truth'], \n",
    "                df['predicted'], \n",
    "                labels=['A', 'B'],\n",
    "                output_dict=True\n",
    "            )\n",
    "            metrics['classification_report'] = report\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def print_metrics(self, metrics: Dict):\n",
    "        \"\"\"\n",
    "        Print formatted evaluation metrics.\n",
    "        \n",
    "        Args:\n",
    "            metrics: Dictionary of metrics to print\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        #print(f\"\\nOverall Accuracy: {metrics['overall_accuracy']:.2%}\")\n",
    "        print(f\"Total Pairs Evaluated: {metrics['total_evaluated']}\")\n",
    "        print(f\"Total Errors: {metrics['total_errors']}\")\n",
    "        \n",
    "        print(\"\\n--- User Metrics ---\")\n",
    "        user_m = metrics['user_metrics']\n",
    "        print(f\"Unique Users: {user_m['unique_users']}\")\n",
    "        print(f\"Avg Accuracy per User: {user_m['avg_accuracy_per_user']:.2%}\")\n",
    "        print(f\"Std Accuracy per User: {user_m['std_accuracy_per_user']:.2%}\")\n",
    "        print(f\"Min User Accuracy: {user_m['min_user_accuracy']:.2%}\")\n",
    "        print(f\"Max User Accuracy: {user_m['max_user_accuracy']:.2%}\")\n",
    "        \n",
    "        print(\"\\n--- Accuracy by Rating Difference ---\")\n",
    "        diff_m = metrics['rating_difference_metrics']\n",
    "        for diff in [1, 2, 3, 4]:\n",
    "            key = f'accuracy_when_diff_{diff}'\n",
    "            if diff_m.get(key) is not None:\n",
    "                print(f\"Rating Diff = {diff}: {diff_m[key]:.2%}\")\n",
    "        \n",
    "        print(\"\\n--- Performance Metrics ---\")\n",
    "        perf_m = metrics['performance_metrics']\n",
    "        print(f\"Avg Inference Time: {perf_m['avg_inference_time']:.2f}s\")\n",
    "        print(f\"Median Inference Time: {perf_m['median_inference_time']:.2f}s\")\n",
    "        print(f\"Avg Response Length: {perf_m['avg_response_length']:.0f} chars\")\n",
    "        print(f\"Avg Prompt Length: {perf_m['avg_prompt_length']:.0f} chars\")\n",
    "        \n",
    "        print(\"\\n--- Prediction Distribution ---\")\n",
    "        pred_d = metrics['prediction_distribution']\n",
    "        print(f\"Predicted A: {pred_d['predicted_A']} ({pred_d['predicted_A']/metrics['total_evaluated']:.1%})\")\n",
    "        print(f\"Predicted B: {pred_d['predicted_B']} ({pred_d['predicted_B']/metrics['total_evaluated']:.1%})\")\n",
    "        \n",
    "        if 'confusion_matrix' in metrics:\n",
    "            print(\"\\n--- Confusion Matrix ---\")\n",
    "            cm = metrics['confusion_matrix']\n",
    "            print(\"           Predicted\")\n",
    "            print(\"           A      B\")\n",
    "            print(f\"True A:  {cm['true_A_pred_A']:4d}  {cm['true_A_pred_B']:4d}\")\n",
    "            print(f\"True B:  {cm['true_B_pred_A']:4d}  {cm['true_B_pred_B']:4d}\")\n",
    "    \n",
    "    def _save_intermediate_results(self, output_dir: str):\n",
    "        \"\"\"Save intermediate results to files.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save results\n",
    "        if self.results:\n",
    "            results_file = Path(output_dir) / f\"results_{timestamp}.json\"\n",
    "            with open(results_file, 'w') as f:\n",
    "                json.dump(self.results, f, indent=2)\n",
    "        \n",
    "        # Save errors\n",
    "        if self.errors:\n",
    "            errors_file = Path(output_dir) / f\"errors_{timestamp}.json\"\n",
    "            with open(errors_file, 'w') as f:\n",
    "                json.dump(self.errors, f, indent=2)\n",
    "        \n",
    "        # Save metrics\n",
    "        if self.results:\n",
    "            metrics = self.calculate_metrics()\n",
    "            metrics_file = Path(output_dir) / f\"metrics_{timestamp}.json\"\n",
    "            with open(metrics_file, 'w') as f:\n",
    "                json.dump(metrics, f, indent=2, default=str)\n",
    "    \n",
    "    def analyze_failures(self, n_examples: int = 5) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze failure cases to understand where the model fails.\n",
    "        \n",
    "        Args:\n",
    "            n_examples: Number of example failures to show\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with failure analysis\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            return {'error': 'No results to analyze'}\n",
    "        \n",
    "        df = pd.DataFrame(self.results)\n",
    "        failures = df[~df['correct']]\n",
    "        \n",
    "        if failures.empty:\n",
    "            return {'message': 'No failures found!'}\n",
    "        \n",
    "        analysis = {\n",
    "            'total_failures': len(failures),\n",
    "            'failure_rate': len(failures) / len(df),\n",
    "            'failures_by_rating_diff': failures.groupby('rating_a').size().to_dict(),\n",
    "            'example_failures': []\n",
    "        }\n",
    "        \n",
    "        # Get example failures\n",
    "        for _, row in failures.head(n_examples).iterrows():\n",
    "            example = {\n",
    "                'user_id': row['user_id'],\n",
    "                'predicted': row['predicted'],\n",
    "                'should_be': row['ground_truth'],\n",
    "                'product_a': f\"{row['product_a']} (rating: {row['rating_a']})\",\n",
    "                'product_b': f\"{row['product_b']} (rating: {row['rating_b']})\",\n",
    "                'response_snippet': row['raw_response'][:200] + \"...\" if len(row['raw_response']) > 200 else row['raw_response']\n",
    "            }\n",
    "            analysis['example_failures'].append(example)\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the evaluation.\n",
    "    \"\"\"\n",
    "    # Load training data\n",
    "    print(\"Loading training data...\")\n",
    "    train_df = pd.read_csv('input/beauty_split_train.csv')\n",
    "    print(f\"Loaded {len(train_df)} training reviews from {train_df['user_id'].nunique()} users\")\n",
    "    \n",
    "    # Initialize evaluator\n",
    "    print(\"\\nInitializing evaluator...\")\n",
    "    evaluator = BeautyPreferenceLLMEvaluator(\n",
    "        train_df=train_df,\n",
    "        test_pairs_file='input/beauty_split_test_pairs.json',\n",
    "        include_product_features=True,\n",
    "        include_review_text=True,\n",
    "        reasoning=True,  # Request reasoning from LLM\n",
    "        temperature=0.0,  # Deterministic for evaluation\n",
    "        max_new_tokens=2560,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(\"\\nStarting evaluation...\")\n",
    "    print(\"This may take a while depending on the number of test pairs and LLM speed.\\n\")\n",
    "    \n",
    "    # Evaluate a sample first to test\n",
    "    print(\"Running initial test on 1 pairs...\")\n",
    "    test_metrics = evaluator.evaluate_batch(\n",
    "        sample_size=1,\n",
    "        verbose=True,\n",
    "        save_intermediate=True,\n",
    "        output_dir='evaluation_results_test'\n",
    "    )\n",
    "    \n",
    "    # Ask user if they want to continue with full evaluation\n",
    "    response = input(\"\\nTest complete. Run full evaluation? (y/n): \")\n",
    "    if response.lower() == 'y':\n",
    "        # Clear previous results\n",
    "        evaluator.results = []\n",
    "        evaluator.errors = []\n",
    "        \n",
    "        # Run full evaluation\n",
    "        print(\"\\nRunning full evaluation...\")\n",
    "        metrics = evaluator.evaluate_batch(\n",
    "            sample_size=100,  # Evaluate all pairs\n",
    "            verbose=True,\n",
    "            save_intermediate=True,\n",
    "            output_dir='evaluation_results_full'\n",
    "        )\n",
    "        \n",
    "        # Analyze failures\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FAILURE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        failure_analysis = evaluator.analyze_failures(n_examples=5)\n",
    "        \n",
    "        if 'example_failures' in failure_analysis:\n",
    "            print(f\"\\nTotal Failures: {failure_analysis['total_failures']}\")\n",
    "            print(f\"Failure Rate: {failure_analysis['failure_rate']:.2%}\")\n",
    "            \n",
    "            print(\"\\nExample Failures:\")\n",
    "            for i, example in enumerate(failure_analysis['example_failures'], 1):\n",
    "                print(f\"\\n{i}. User: {example['user_id']}\")\n",
    "                print(f\"   Predicted: {example['predicted']}, Should be: {example['should_be']}\")\n",
    "                print(f\"   Product A: {example['product_a']}\")\n",
    "                print(f\"   Product B: {example['product_b']}\")\n",
    "                print(f\"   Response: {example['response_snippet']}\")\n",
    "        \n",
    "        # Save final results\n",
    "        print(\"\\nSaving final results...\")\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        final_results = {\n",
    "            'metrics': metrics,\n",
    "            'failure_analysis': failure_analysis,\n",
    "            'evaluation_summary': {\n",
    "                'total_pairs_evaluated': len(evaluator.results),\n",
    "                'overall_accuracy': metrics['overall_accuracy'],\n",
    "                'timestamp': timestamp,\n",
    "                'model_used': evaluator.llm_client.model_id\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(f'evaluation_results_full/final_summary_{timestamp}.json', 'w') as f:\n",
    "            json.dump(final_results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"Results saved to evaluation_results_full/final_summary_{timestamp}.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import asyncio\n",
    "\n",
    "# Import the prompt template (assumes it's in the same directory or installed)\n",
    "from prompt import BeautyPromptTemplateAdapter\n",
    "\n",
    "# Import the LLM client\n",
    "from remoteOss import get_local_client\n",
    "\n",
    "\n",
    "class BeautyPreferenceLLMEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluates LLM performance on beauty product preference prediction with batch processing support.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        test_pairs_file: str = 'input/beauty_split_test_pairs.json',\n",
    "        model_id: str = None,\n",
    "        include_product_features: bool = True,\n",
    "        include_review_text: bool = True,\n",
    "        reasoning: bool = True,\n",
    "        max_review_text_length: int = 150,\n",
    "        temperature: float = 0.0,\n",
    "        max_new_tokens: int = 256,\n",
    "        seed: int = 42,\n",
    "        batch_size: int = 32,\n",
    "        max_concurrent_batches: int = 2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with batch processing support.\n",
    "        \n",
    "        Args:\n",
    "            train_df: Training DataFrame with user reviews\n",
    "            test_pairs_file: Path to test pairs JSON file\n",
    "            model_id: Model ID for the LLM (None uses default)\n",
    "            include_product_features: Include features in prompts\n",
    "            include_review_text: Include review text in prompts\n",
    "            reasoning: Request reasoning from the LLM\n",
    "            max_review_text_length: Max length for review text\n",
    "            temperature: LLM temperature setting\n",
    "            max_new_tokens: Max tokens for LLM response\n",
    "            seed: Random seed for reproducibility\n",
    "            batch_size: Number of queries to process in a single batch\n",
    "            max_concurrent_batches: Maximum number of concurrent batch requests\n",
    "        \"\"\"\n",
    "        # Initialize prompt template\n",
    "        self.prompt_template = BeautyPromptTemplateAdapter(\n",
    "            train_df=train_df,\n",
    "            test_pairs_file=test_pairs_file,\n",
    "            include_product_features=include_product_features,\n",
    "            include_review_text=include_review_text,\n",
    "            reasoning=reasoning,\n",
    "            max_review_text_length=max_review_text_length\n",
    "        )\n",
    "        \n",
    "        # Initialize LLM client\n",
    "        if model_id:\n",
    "            self.llm_client = get_local_client(model_id=model_id)\n",
    "        else:\n",
    "            self.llm_client = get_local_client()\n",
    "        \n",
    "        # LLM parameters\n",
    "        self.temperature = temperature\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Batch processing parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.max_concurrent_batches = max_concurrent_batches\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = []\n",
    "        self.errors = []\n",
    "    \n",
    "    def prepare_batch_data(self, pair_indices: List[int]) -> Tuple[List[str], List[Dict]]:\n",
    "        \"\"\"\n",
    "        Prepare batch data for multiple test pairs.\n",
    "        \n",
    "        Args:\n",
    "            pair_indices: List of test pair indices\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (prompts, pair_infos) for the batch\n",
    "        \"\"\"\n",
    "        prompts = []\n",
    "        pair_infos = []\n",
    "        \n",
    "        for idx in pair_indices:\n",
    "            try:\n",
    "                prompt, pair_info = self.prompt_template.format_from_test_pair(idx)\n",
    "                eval_info = self.prompt_template.evaluate_pair(idx)\n",
    "                \n",
    "                # Combine pair_info with eval_info for complete context\n",
    "                complete_info = {\n",
    "                    'pair_index': idx,\n",
    "                    'prompt': prompt,\n",
    "                    **pair_info,\n",
    "                    **eval_info\n",
    "                }\n",
    "                \n",
    "                prompts.append(prompt)\n",
    "                pair_infos.append(complete_info)\n",
    "            except Exception as e:\n",
    "                self.errors.append({\n",
    "                    'pair_index': idx,\n",
    "                    'error': f\"Error preparing pair: {str(e)}\",\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        return prompts, pair_infos\n",
    "    \n",
    "    def process_batch_responses(\n",
    "        self, \n",
    "        responses: List[Tuple[str, str]], \n",
    "        pair_infos: List[Dict],\n",
    "        inference_time: float\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process batch responses from the LLM.\n",
    "        \n",
    "        Args:\n",
    "            responses: List of (choice, raw_response) tuples from LLM\n",
    "            pair_infos: List of pair information dictionaries\n",
    "            inference_time: Total inference time for the batch\n",
    "            \n",
    "        Returns:\n",
    "            List of result dictionaries\n",
    "        \"\"\"\n",
    "        batch_results = []\n",
    "        avg_inference_time = inference_time / len(responses) if responses else 0\n",
    "        \n",
    "        for (choice, raw_response), pair_info in zip(responses, pair_infos):\n",
    "            try:\n",
    "                # Check if prediction is correct\n",
    "                is_correct = (choice == pair_info['ground_truth'])\n",
    "                \n",
    "                result = {\n",
    "                    'pair_index': pair_info['pair_index'],\n",
    "                    'user_id': pair_info['user_id'],\n",
    "                    'predicted': choice,\n",
    "                    'ground_truth': pair_info['ground_truth'],\n",
    "                    'correct': is_correct,\n",
    "                    'product_a': pair_info['product_a_title'],\n",
    "                    'product_b': pair_info['product_b_title'],\n",
    "                    'rating_a': pair_info['product_a_rating'],\n",
    "                    'rating_b': pair_info['product_b_rating'],\n",
    "                    'raw_response': raw_response,\n",
    "                    'inference_time': avg_inference_time,\n",
    "                    'prompt_length': len(pair_info['prompt']),\n",
    "                    'response_length': len(raw_response)\n",
    "                }\n",
    "                \n",
    "                batch_results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.errors.append({\n",
    "                    'pair_index': pair_info.get('pair_index', -1),\n",
    "                    'error': f\"Error processing response: {str(e)}\",\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        return batch_results\n",
    "    \n",
    "    def call_batch_oracle(self, prompts: List[str], pair_infos: List[Dict]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Call the LLM with a batch of prompts.\n",
    "        \n",
    "        Args:\n",
    "            prompts: List of prompts to process\n",
    "            pair_infos: List of pair information for context\n",
    "            \n",
    "        Returns:\n",
    "            List of (choice, raw_response) tuples\n",
    "        \"\"\"\n",
    "        # Check if the LLM client supports batch processing\n",
    "        if hasattr(self.llm_client, 'call_batch_oracle'):\n",
    "            # Use batch method if available\n",
    "            schedules_a = []\n",
    "            schedules_b = []\n",
    "            \n",
    "            for info in pair_infos:\n",
    "                sched_a = {\n",
    "                    'product': info['product_a_title'],\n",
    "                    'rating': info['product_a_rating']\n",
    "                }\n",
    "                sched_b = {\n",
    "                    'product': info['product_b_title'],\n",
    "                    'rating': info['product_b_rating']\n",
    "                }\n",
    "                schedules_a.append(sched_a)\n",
    "                schedules_b.append(sched_b)\n",
    "            \n",
    "            return self.llm_client.call_batch_oracle(\n",
    "                prompts=prompts,\n",
    "                schedules_a=schedules_a,\n",
    "                schedules_b=schedules_b,\n",
    "                temperature=self.temperature,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                seed=self.seed,\n",
    "                stop=[\"===\", \"---\", \"\\n\\n\\n\"]\n",
    "            )\n",
    "        else:\n",
    "            # Fallback to sequential processing if batch method not available\n",
    "            responses = []\n",
    "            for prompt, info in zip(prompts, pair_infos):\n",
    "                sched_a = {\n",
    "                    'product': info['product_a_title'],\n",
    "                    'rating': info['product_a_rating']\n",
    "                }\n",
    "                sched_b = {\n",
    "                    'product': info['product_b_title'],\n",
    "                    'rating': info['product_b_rating']\n",
    "                }\n",
    "                \n",
    "                choice, raw_response = self.llm_client.call_oracle(\n",
    "                    prompt=prompt,\n",
    "                    sched_a=sched_a,\n",
    "                    sched_b=sched_b,\n",
    "                    temperature=self.temperature,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    seed=self.seed,\n",
    "                    stop=[\"===\", \"---\", \"\\n\\n\\n\"]\n",
    "                )\n",
    "                responses.append((choice, raw_response))\n",
    "            \n",
    "            return responses\n",
    "    \n",
    "    def evaluate_batch_concurrent(\n",
    "        self,\n",
    "        pair_indices: List[int] = None,\n",
    "        sample_size: int = None,\n",
    "        verbose: bool = True,\n",
    "        save_intermediate: bool = True,\n",
    "        output_dir: str = 'evaluation_results'\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate multiple test pairs using batch processing with concurrency.\n",
    "        \n",
    "        Args:\n",
    "            pair_indices: Specific indices to evaluate\n",
    "            sample_size: Random sample size (if pair_indices not provided)\n",
    "            verbose: Print progress\n",
    "            save_intermediate: Save results periodically\n",
    "            output_dir: Directory to save results\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        # Determine which pairs to evaluate\n",
    "        if pair_indices is None:\n",
    "            total_pairs = len(self.prompt_template.test_pairs)\n",
    "            if sample_size:\n",
    "                import random\n",
    "                pair_indices = random.sample(range(total_pairs), min(sample_size, total_pairs))\n",
    "            else:\n",
    "                pair_indices = list(range(total_pairs))\n",
    "        else:\n",
    "            pair_indices = list(pair_indices)\n",
    "        \n",
    "        # Create output directory\n",
    "        if save_intermediate:\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Split into batches\n",
    "        batches = [pair_indices[i:i + self.batch_size] \n",
    "                  for i in range(0, len(pair_indices), self.batch_size)]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Processing {len(pair_indices)} pairs in {len(batches)} batches of size {self.batch_size}\")\n",
    "        \n",
    "        # Process batches with concurrency control\n",
    "        with ThreadPoolExecutor(max_workers=self.max_concurrent_batches) as executor:\n",
    "            futures = []\n",
    "            \n",
    "            # Submit batches for processing\n",
    "            for batch_idx, batch_indices in enumerate(batches):\n",
    "                future = executor.submit(self._process_single_batch, batch_indices, batch_idx)\n",
    "                futures.append(future)\n",
    "            \n",
    "            # Process completed batches\n",
    "            for future in tqdm(as_completed(futures), total=len(futures), \n",
    "                             desc=\"Processing batches\", disable=not verbose):\n",
    "                try:\n",
    "                    batch_results = future.result()\n",
    "                    self.results.extend(batch_results)\n",
    "                    \n",
    "                    # Save intermediate results\n",
    "                    if save_intermediate and len(self.results) % (self.batch_size * 2) == 0:\n",
    "                        self._save_intermediate_results(output_dir)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Batch processing error: {e}\")\n",
    "        \n",
    "        # Final save\n",
    "        if save_intermediate:\n",
    "            self._save_intermediate_results(output_dir)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self.calculate_metrics()\n",
    "        \n",
    "        if verbose:\n",
    "            self.print_metrics(metrics)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _process_single_batch(self, batch_indices: List[int], batch_idx: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process a single batch of test pairs.\n",
    "        \n",
    "        Args:\n",
    "            batch_indices: Indices for this batch\n",
    "            batch_idx: Batch number (for logging)\n",
    "            \n",
    "        Returns:\n",
    "            List of results for this batch\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prepare batch data\n",
    "            prompts, pair_infos = self.prepare_batch_data(batch_indices)\n",
    "            print('prompts' , prompts )\n",
    "            if not prompts:\n",
    "                return []\n",
    "            \n",
    "            # Call LLM with batch\n",
    "            start_time = time.time()\n",
    "            responses = self.call_batch_oracle(prompts, pair_infos)\n",
    "            inference_time = time.time() - start_time\n",
    "            print('responses ' , responses)\n",
    "            # Process responses\n",
    "            batch_results = self.process_batch_responses(responses, pair_infos, inference_time)\n",
    "            \n",
    "            return batch_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {batch_idx}: {e}\")\n",
    "            self.errors.append({\n",
    "                'batch_idx': batch_idx,\n",
    "                'batch_indices': batch_indices,\n",
    "                'error': str(e),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            return []\n",
    "    \n",
    "    def calculate_metrics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate evaluation metrics from results.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with various metrics\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            return {'error': 'No results to evaluate'}\n",
    "        \n",
    "        df = pd.DataFrame(self.results)\n",
    "        \n",
    "        # Basic accuracy\n",
    "        accuracy = df['correct'].mean()\n",
    "        \n",
    "        # Per-user accuracy\n",
    "        user_accuracy = df.groupby('user_id')['correct'].agg(['mean', 'count'])\n",
    "        \n",
    "        # Rating difference analysis\n",
    "        df['rating_diff'] = df['rating_a'] - df['rating_b']\n",
    "        \n",
    "        # Accuracy by rating difference magnitude\n",
    "        df['rating_diff_abs'] = df['rating_diff'].abs()\n",
    "        accuracy_by_diff = df.groupby('rating_diff_abs')['correct'].agg(['mean', 'count'])\n",
    "        \n",
    "        # Inference time statistics\n",
    "        avg_inference_time = df['inference_time'].mean()\n",
    "        median_inference_time = df['inference_time'].median()\n",
    "        \n",
    "        # Response analysis\n",
    "        avg_response_length = df['response_length'].mean()\n",
    "        avg_prompt_length = df['prompt_length'].mean()\n",
    "        \n",
    "        metrics = {\n",
    "            'overall_accuracy': accuracy,\n",
    "            'total_evaluated': len(df),\n",
    "            'total_errors': len(self.errors),\n",
    "            'batch_size_used': self.batch_size,\n",
    "            'max_concurrent_batches': self.max_concurrent_batches,\n",
    "            'user_metrics': {\n",
    "                'unique_users': df['user_id'].nunique(),\n",
    "                'avg_accuracy_per_user': user_accuracy['mean'].mean(),\n",
    "                'std_accuracy_per_user': user_accuracy['mean'].std(),\n",
    "                'min_user_accuracy': user_accuracy['mean'].min(),\n",
    "                'max_user_accuracy': user_accuracy['mean'].max()\n",
    "            },\n",
    "            'rating_difference_metrics': {\n",
    "                'accuracy_by_diff': accuracy_by_diff.to_dict() if not accuracy_by_diff.empty else {},\n",
    "                'avg_rating_diff': df['rating_diff'].mean(),\n",
    "                'accuracy_when_diff_1': df[df['rating_diff_abs'] == 1]['correct'].mean() if len(df[df['rating_diff_abs'] == 1]) > 0 else None,\n",
    "                'accuracy_when_diff_2': df[df['rating_diff_abs'] == 2]['correct'].mean() if len(df[df['rating_diff_abs'] == 2]) > 0 else None,\n",
    "                'accuracy_when_diff_3': df[df['rating_diff_abs'] == 3]['correct'].mean() if len(df[df['rating_diff_abs'] == 3]) > 0 else None,\n",
    "                'accuracy_when_diff_4': df[df['rating_diff_abs'] == 4]['correct'].mean() if len(df[df['rating_diff_abs'] == 4]) > 0 else None,\n",
    "            },\n",
    "            'performance_metrics': {\n",
    "                'avg_inference_time_per_item': avg_inference_time,\n",
    "                'median_inference_time_per_item': median_inference_time,\n",
    "                'total_inference_time': df['inference_time'].sum(),\n",
    "                'avg_response_length': avg_response_length,\n",
    "                'avg_prompt_length': avg_prompt_length,\n",
    "                'throughput_items_per_second': len(df) / df['inference_time'].sum() if df['inference_time'].sum() > 0 else 0\n",
    "            },\n",
    "            'prediction_distribution': {\n",
    "                'predicted_A': (df['predicted'] == 'A').sum(),\n",
    "                'predicted_B': (df['predicted'] == 'B').sum(),\n",
    "                'ground_truth_A': (df['ground_truth'] == 'A').sum(),\n",
    "                'ground_truth_B': (df['ground_truth'] == 'B').sum()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add confusion matrix\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "        \n",
    "        try:\n",
    "            cm = confusion_matrix(df['ground_truth'], df['predicted'], labels=['A', 'B'])\n",
    "            metrics['confusion_matrix'] = {\n",
    "                'true_A_pred_A': int(cm[0, 0]),\n",
    "                'true_A_pred_B': int(cm[0, 1]),\n",
    "                'true_B_pred_A': int(cm[1, 0]),\n",
    "                'true_B_pred_B': int(cm[1, 1])\n",
    "            }\n",
    "            \n",
    "            # Add classification report\n",
    "            report = classification_report(\n",
    "                df['ground_truth'], \n",
    "                df['predicted'], \n",
    "                labels=['A', 'B'],\n",
    "                output_dict=True\n",
    "            )\n",
    "            metrics['classification_report'] = report\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def print_metrics(self, metrics: Dict):\n",
    "        \"\"\"\n",
    "        Print formatted evaluation metrics.\n",
    "        \n",
    "        Args:\n",
    "            metrics: Dictionary of metrics to print\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nOverall Accuracy: {metrics['overall_accuracy']:.2%}\")\n",
    "        print(f\"Total Pairs Evaluated: {metrics['total_evaluated']}\")\n",
    "        print(f\"Total Errors: {metrics['total_errors']}\")\n",
    "        print(f\"Batch Size: {metrics['batch_size_used']}\")\n",
    "        print(f\"Max Concurrent Batches: {metrics['max_concurrent_batches']}\")\n",
    "        \n",
    "        print(\"\\n--- User Metrics ---\")\n",
    "        user_m = metrics['user_metrics']\n",
    "        print(f\"Unique Users: {user_m['unique_users']}\")\n",
    "        print(f\"Avg Accuracy per User: {user_m['avg_accuracy_per_user']:.2%}\")\n",
    "        print(f\"Std Accuracy per User: {user_m['std_accuracy_per_user']:.2%}\")\n",
    "        print(f\"Min User Accuracy: {user_m['min_user_accuracy']:.2%}\")\n",
    "        print(f\"Max User Accuracy: {user_m['max_user_accuracy']:.2%}\")\n",
    "        \n",
    "        print(\"\\n--- Accuracy by Rating Difference ---\")\n",
    "        diff_m = metrics['rating_difference_metrics']\n",
    "        for diff in [1, 2, 3, 4]:\n",
    "            key = f'accuracy_when_diff_{diff}'\n",
    "            if diff_m.get(key) is not None:\n",
    "                print(f\"Rating Diff = {diff}: {diff_m[key]:.2%}\")\n",
    "        \n",
    "        print(\"\\n--- Performance Metrics ---\")\n",
    "        perf_m = metrics['performance_metrics']\n",
    "        print(f\"Avg Inference Time per Item: {perf_m['avg_inference_time_per_item']:.3f}s\")\n",
    "        print(f\"Median Inference Time per Item: {perf_m['median_inference_time_per_item']:.3f}s\")\n",
    "        print(f\"Total Inference Time: {perf_m['total_inference_time']:.1f}s\")\n",
    "        print(f\"Throughput: {perf_m['throughput_items_per_second']:.1f} items/second\")\n",
    "        print(f\"Avg Response Length: {perf_m['avg_response_length']:.0f} chars\")\n",
    "        print(f\"Avg Prompt Length: {perf_m['avg_prompt_length']:.0f} chars\")\n",
    "        \n",
    "        print(\"\\n--- Prediction Distribution ---\")\n",
    "        pred_d = metrics['prediction_distribution']\n",
    "        print(f\"Predicted A: {pred_d['predicted_A']} ({pred_d['predicted_A']/metrics['total_evaluated']:.1%})\")\n",
    "        print(f\"Predicted B: {pred_d['predicted_B']} ({pred_d['predicted_B']/metrics['total_evaluated']:.1%})\")\n",
    "        \n",
    "        if 'confusion_matrix' in metrics:\n",
    "            print(\"\\n--- Confusion Matrix ---\")\n",
    "            cm = metrics['confusion_matrix']\n",
    "            print(\"           Predicted\")\n",
    "            print(\"           A      B\")\n",
    "            print(f\"True A:  {cm['true_A_pred_A']:4d}  {cm['true_A_pred_B']:4d}\")\n",
    "            print(f\"True B:  {cm['true_B_pred_A']:4d}  {cm['true_B_pred_B']:4d}\")\n",
    "    \n",
    "    def _save_intermediate_results(self, output_dir: str):\n",
    "        \"\"\"Save intermediate results to files.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Save results\n",
    "        if self.results:\n",
    "            results_file = Path(output_dir) / f\"results_{timestamp}.json\"\n",
    "            with open(results_file, 'w') as f:\n",
    "                json.dump(self.results, f, indent=2)\n",
    "        \n",
    "        # Save errors\n",
    "        if self.errors:\n",
    "            errors_file = Path(output_dir) / f\"errors_{timestamp}.json\"\n",
    "            with open(errors_file, 'w') as f:\n",
    "                json.dump(self.errors, f, indent=2)\n",
    "        \n",
    "        # Save metrics\n",
    "        if self.results:\n",
    "            metrics = self.calculate_metrics()\n",
    "            metrics_file = Path(output_dir) / f\"metrics_{timestamp}.json\"\n",
    "            with open(metrics_file, 'w') as f:\n",
    "                json.dump(metrics, f, indent=2, default=str)\n",
    "    \n",
    "    def analyze_failures(self, n_examples: int = 5) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze failure cases to understand where the model fails.\n",
    "        \n",
    "        Args:\n",
    "            n_examples: Number of example failures to show\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with failure analysis\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            return {'error': 'No results to analyze'}\n",
    "        \n",
    "        df = pd.DataFrame(self.results)\n",
    "        failures = df[~df['correct']]\n",
    "        \n",
    "        if failures.empty:\n",
    "            return {'message': 'No failures found!'}\n",
    "        \n",
    "        analysis = {\n",
    "            'total_failures': len(failures),\n",
    "            'failure_rate': len(failures) / len(df),\n",
    "            'failures_by_rating_diff': failures.groupby('rating_a').size().to_dict(),\n",
    "            'example_failures': []\n",
    "        }\n",
    "        \n",
    "        # Get example failures\n",
    "        for _, row in failures.head(n_examples).iterrows():\n",
    "            example = {\n",
    "                'user_id': row['user_id'],\n",
    "                'predicted': row['predicted'],\n",
    "                'should_be': row['ground_truth'],\n",
    "                'product_a': f\"{row['product_a']} (rating: {row['rating_a']})\",\n",
    "                'product_b': f\"{row['product_b']} (rating: {row['rating_b']})\",\n",
    "                'response_snippet': row['raw_response'][:200] + \"...\" if len(row['raw_response']) > 200 else row['raw_response']\n",
    "            }\n",
    "            analysis['example_failures'].append(example)\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the batch evaluation.\n",
    "    \"\"\"\n",
    "    # Load training data\n",
    "    print(\"Loading training data...\")\n",
    "    train_df = pd.read_csv('input/beauty_split_train.csv')\n",
    "    print(f\"Loaded {len(train_df)} training reviews from {train_df['user_id'].nunique()} users\")\n",
    "    \n",
    "    # Initialize evaluator with batch processing\n",
    "    print(\"\\nInitializing evaluator with batch processing...\")\n",
    "    evaluator = BeautyPreferenceLLMEvaluator(\n",
    "        train_df=train_df,\n",
    "        test_pairs_file='input/beauty_split_test_pairs.json',\n",
    "        include_product_features=True,\n",
    "        include_review_text=True,\n",
    "        reasoning=True,\n",
    "        temperature=0.0,\n",
    "        max_new_tokens=2048,\n",
    "        seed=42,\n",
    "        batch_size=32,  # Process 32 queries at once\n",
    "        max_concurrent_batches=2  # Allow 2 concurrent batch requests\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(\"\\nStarting batch evaluation...\")\n",
    "    print(\"This should be significantly faster with batch processing.\\n\")\n",
    "    \n",
    "    # Test with a small batch first\n",
    "    print(\"Running initial test on 10 pairs...\")\n",
    "    test_metrics = evaluator.evaluate_batch_concurrent(\n",
    "        sample_size=10,\n",
    "        verbose=True,\n",
    "        save_intermediate=True,\n",
    "        output_dir='evaluation_results_test'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTest batch completed. Throughput: {test_metrics['performance_metrics']['throughput_items_per_second']:.1f} items/second\")\n",
    "    \n",
    "    # Ask user if they want to continue with full evaluation\n",
    "    response = input(\"\\nTest complete. Run full evaluation? (y/n): \")\n",
    "    if response.lower() == 'y':\n",
    "        # Clear previous results\n",
    "        evaluator.results = []\n",
    "        evaluator.errors = []\n",
    "        \n",
    "        # You can adjust batch size based on your VLLM server capacity\n",
    "        batch_size = input(\"Enter batch size (default 32, max recommended 128): \").strip()\n",
    "        if batch_size.isdigit():\n",
    "            evaluator.batch_size = int(batch_size)\n",
    "        \n",
    "        concurrent = input(\"Enter max concurrent batches (default 2): \").strip()\n",
    "        if concurrent.isdigit():\n",
    "            evaluator.max_concurrent_batches = int(concurrent)\n",
    "        \n",
    "        # Run full evaluation\n",
    "        print(f\"\\nRunning full evaluation with batch_size={evaluator.batch_size}, max_concurrent={evaluator.max_concurrent_batches}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        metrics = evaluator.evaluate_batch_concurrent(\n",
    "            sample_size=320,  # Evaluate all pairs\n",
    "            verbose=True,\n",
    "            save_intermediate=True,\n",
    "            output_dir='evaluation_results_full'\n",
    "        )\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nTotal evaluation time: {total_time:.1f} seconds\")\n",
    "        print(f\"Average throughput: {metrics['performance_metrics']['throughput_items_per_second']:.1f} items/second\")\n",
    "        \n",
    "        # Analyze failures\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FAILURE ANALYSIS\")\n",
    "        print(\"=\"*60)\n",
    "        failure_analysis = evaluator.analyze_failures(n_examples=5)\n",
    "        \n",
    "        if 'example_failures' in failure_analysis:\n",
    "            print(f\"\\nTotal Failures: {failure_analysis['total_failures']}\")\n",
    "            print(f\"Failure Rate: {failure_analysis['failure_rate']:.2%}\")\n",
    "            \n",
    "            print(\"\\nExample Failures:\")\n",
    "            for i, example in enumerate(failure_analysis['example_failures'], 1):\n",
    "                print(f\"\\n{i}. User: {example['user_id']}\")\n",
    "                print(f\"   Predicted: {example['predicted']}, Should be: {example['should_be']}\")\n",
    "                print(f\"   Product A: {example['product_a']}\")\n",
    "                print(f\"   Product B: {example['product_b']}\")\n",
    "                print(f\"   Response: {example['response_snippet']}\")\n",
    "        \n",
    "        # Save final results\n",
    "        print(\"\\nSaving final results...\")\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        final_results = {\n",
    "            'metrics': metrics,\n",
    "            'failure_analysis': failure_analysis,\n",
    "            'evaluation_summary': {\n",
    "                'total_pairs_evaluated': len(evaluator.results),\n",
    "                'overall_accuracy': metrics['overall_accuracy'],\n",
    "                'total_evaluation_time': total_time,\n",
    "                'throughput_items_per_second': metrics['performance_metrics']['throughput_items_per_second'],\n",
    "                'batch_size': evaluator.batch_size,\n",
    "                'max_concurrent_batches': evaluator.max_concurrent_batches,\n",
    "                'timestamp': timestamp,\n",
    "                'model_used': evaluator.llm_client.model_id if hasattr(evaluator.llm_client, 'model_id') else 'unknown'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(f'evaluation_results_full/final_summary_{timestamp}.json', 'w') as f:\n",
    "            json.dump(final_results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"Results saved to evaluation_results_full/final_summary_{timestamp}.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea82a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_results(filepath):\n",
    "    \"\"\"Load the JSON results file.\"\"\"\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def compute_metrics(data):\n",
    "    \"\"\"Compute various evaluation metrics from the results.\"\"\"\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Basic accuracy\n",
    "    accuracy = df['correct'].mean()\n",
    "    total_predictions = len(df)\n",
    "    correct_predictions = df['correct'].sum()\n",
    "    \n",
    "    # Get predictions and ground truth\n",
    "    y_true = df['ground_truth'].values\n",
    "    y_pred = df['predicted'].values\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=['A', 'B'])\n",
    "    \n",
    "    # Classification report\n",
    "    class_report = classification_report(y_true, y_pred, labels=['A', 'B'], output_dict=True)\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    class_a_mask = df['ground_truth'] == 'A'\n",
    "    class_b_mask = df['ground_truth'] == 'B'\n",
    "    \n",
    "    accuracy_a = df[class_a_mask]['correct'].mean() if class_a_mask.any() else 0\n",
    "    accuracy_b = df[class_b_mask]['correct'].mean() if class_b_mask.any() else 0\n",
    "    \n",
    "    # Rating difference analysis\n",
    "    df['rating_diff'] = df['rating_a'] - df['rating_b']\n",
    "    df['abs_rating_diff'] = df['rating_diff'].abs()\n",
    "    \n",
    "    # Accuracy by rating difference magnitude\n",
    "    rating_bins = [0, 1, 2, 3, 4, 5]\n",
    "    df['rating_diff_bin'] = pd.cut(df['abs_rating_diff'], bins=rating_bins, include_lowest=True)\n",
    "    accuracy_by_rating_diff = df.groupby('rating_diff_bin')['correct'].agg(['mean', 'count'])\n",
    "    \n",
    "    # User-level accuracy (if multiple predictions per user)\n",
    "    user_accuracy = df.groupby('user_id')['correct'].mean()\n",
    "    \n",
    "    # Inference time statistics\n",
    "    inference_stats = {\n",
    "        'mean': df['inference_time'].mean(),\n",
    "        'median': df['inference_time'].median(),\n",
    "        'std': df['inference_time'].std(),\n",
    "        'min': df['inference_time'].min(),\n",
    "        'max': df['inference_time'].max()\n",
    "    }\n",
    "    \n",
    "    # Prompt and response length statistics\n",
    "    prompt_stats = {\n",
    "        'mean': df['prompt_length'].mean(),\n",
    "        'median': df['prompt_length'].median(),\n",
    "        'std': df['prompt_length'].std()\n",
    "    }\n",
    "    \n",
    "    response_stats = {\n",
    "        'mean': df['response_length'].mean(),\n",
    "        'median': df['response_length'].median(),\n",
    "        'std': df['response_length'].std()\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'overall_accuracy': accuracy,\n",
    "        'total_predictions': total_predictions,\n",
    "        'correct_predictions': correct_predictions,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': class_report,\n",
    "        'accuracy_class_A': accuracy_a,\n",
    "        'accuracy_class_B': accuracy_b,\n",
    "        'accuracy_by_rating_diff': accuracy_by_rating_diff,\n",
    "        'user_accuracy': user_accuracy,\n",
    "        'inference_time_stats': inference_stats,\n",
    "        'prompt_length_stats': prompt_stats,\n",
    "        'response_length_stats': response_stats,\n",
    "        'dataframe': df\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion Matrix'):\n",
    "    \"\"\"Plot a confusion matrix.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Predicted A', 'Predicted B'],\n",
    "                yticklabels=['True A', 'True B'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy_by_rating_diff(accuracy_by_diff):\n",
    "    \"\"\"Plot accuracy by rating difference.\"\"\"\n",
    "    if not accuracy_by_diff.empty:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Accuracy by rating difference\n",
    "        accuracy_by_diff['mean'].plot(kind='bar', ax=ax1, color='skyblue')\n",
    "        ax1.set_title('Accuracy by Rating Difference')\n",
    "        ax1.set_xlabel('Absolute Rating Difference')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)\n",
    "        \n",
    "        # Sample count by rating difference\n",
    "        accuracy_by_diff['count'].plot(kind='bar', ax=ax2, color='lightcoral')\n",
    "        ax2.set_title('Sample Count by Rating Difference')\n",
    "        ax2.set_xlabel('Absolute Rating Difference')\n",
    "        ax2.set_ylabel('Number of Samples')\n",
    "        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def print_summary(metrics):\n",
    "    \"\"\"Print a summary of the evaluation metrics.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EVALUATION RESULTS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"  Total Predictions: {metrics['total_predictions']}\")\n",
    "    print(f\"  Correct Predictions: {metrics['correct_predictions']}\")\n",
    "    print(f\"  Overall Accuracy: {metrics['overall_accuracy']:.4f} ({metrics['overall_accuracy']*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nPer-Class Accuracy:\")\n",
    "    print(f\"  Class A Accuracy: {metrics['accuracy_class_A']:.4f} ({metrics['accuracy_class_A']*100:.2f}%)\")\n",
    "    print(f\"  Class B Accuracy: {metrics['accuracy_class_B']:.4f} ({metrics['accuracy_class_B']*100:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  Rows: True labels, Columns: Predicted labels\")\n",
    "    cm = metrics['confusion_matrix']\n",
    "    print(f\"           Pred A   Pred B\")\n",
    "    print(f\"  True A:   {cm[0,0]:5d}   {cm[0,1]:5d}\")\n",
    "    print(f\"  True B:   {cm[1,0]:5d}   {cm[1,1]:5d}\")\n",
    "    \n",
    "    print(f\"\\nClassification Report:\")\n",
    "    for class_label in ['A', 'B']:\n",
    "        cr = metrics['classification_report'][class_label]\n",
    "        print(f\"  Class {class_label}:\")\n",
    "        print(f\"    Precision: {cr['precision']:.4f}\")\n",
    "        print(f\"    Recall: {cr['recall']:.4f}\")\n",
    "        print(f\"    F1-Score: {cr['f1-score']:.4f}\")\n",
    "        print(f\"    Support: {cr['support']}\")\n",
    "    \n",
    "    print(f\"\\nInference Time Statistics (seconds):\")\n",
    "    inf_stats = metrics['inference_time_stats']\n",
    "    print(f\"  Mean: {inf_stats['mean']:.2f}\")\n",
    "    print(f\"  Median: {inf_stats['median']:.2f}\")\n",
    "    print(f\"  Std Dev: {inf_stats['std']:.2f}\")\n",
    "    print(f\"  Min: {inf_stats['min']:.2f}\")\n",
    "    print(f\"  Max: {inf_stats['max']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nPrompt Length Statistics:\")\n",
    "    prompt_stats = metrics['prompt_length_stats']\n",
    "    print(f\"  Mean: {prompt_stats['mean']:.0f}\")\n",
    "    print(f\"  Median: {prompt_stats['median']:.0f}\")\n",
    "    print(f\"  Std Dev: {prompt_stats['std']:.0f}\")\n",
    "    \n",
    "    print(f\"\\nResponse Length Statistics:\")\n",
    "    resp_stats = metrics['response_length_stats']\n",
    "    print(f\"  Mean: {resp_stats['mean']:.0f}\")\n",
    "    print(f\"  Median: {resp_stats['median']:.0f}\")\n",
    "    print(f\"  Std Dev: {resp_stats['std']:.0f}\")\n",
    "    \n",
    "    print(f\"\\nAccuracy by Rating Difference:\")\n",
    "    if not metrics['accuracy_by_rating_diff'].empty:\n",
    "        for idx, row in metrics['accuracy_by_rating_diff'].iterrows():\n",
    "            print(f\"  {idx}: {row['mean']:.4f} (n={int(row['count'])})\")\n",
    "    \n",
    "    print(f\"\\nUser-Level Statistics:\")\n",
    "    user_acc = metrics['user_accuracy']\n",
    "    print(f\"  Number of unique users: {len(user_acc)}\")\n",
    "    print(f\"  Mean user accuracy: {user_acc.mean():.4f}\")\n",
    "    print(f\"  Std dev of user accuracy: {user_acc.std():.4f}\")\n",
    "\n",
    "def main():\n",
    "    # Load the results\n",
    "    filepath = '/home/asj53/LISTEN/LISTEN/evaluation_results_full/results_20250824_131702.json'\n",
    "    \n",
    "    try:\n",
    "        data = load_results(filepath)\n",
    "        print(f\"Successfully loaded {len(data)} predictions from {filepath}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON in file {filepath}\")\n",
    "        return\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(data)\n",
    "    \n",
    "    # Print summary\n",
    "    print_summary(metrics)\n",
    "    \n",
    "    # Plot visualizations\n",
    "    plot_confusion_matrix(metrics['confusion_matrix'])\n",
    "    plot_accuracy_by_rating_diff(metrics['accuracy_by_rating_diff'])\n",
    "    \n",
    "    # Additional analysis\n",
    "    df = metrics['dataframe']\n",
    "    \n",
    "    # Analyze errors\n",
    "    errors = df[~df['correct']]\n",
    "    if not errors.empty:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ERROR ANALYSIS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total errors: {len(errors)}\")\n",
    "        print(f\"Error rate: {len(errors)/len(df):.4f}\")\n",
    "        \n",
    "        # Error distribution\n",
    "        error_dist = errors.groupby(['ground_truth', 'predicted']).size()\n",
    "        print(f\"\\nError distribution:\")\n",
    "        for (true_label, pred_label), count in error_dist.items():\n",
    "            print(f\"  True: {true_label}, Predicted: {pred_label}: {count} errors\")\n",
    "        \n",
    "        # Errors by rating difference\n",
    "        print(f\"\\nAverage rating difference for errors: {errors['abs_rating_diff'].mean():.2f}\")\n",
    "        print(f\"Average rating difference for correct: {df[df['correct']]['abs_rating_diff'].mean():.2f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    metrics = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c19a37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo-opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
